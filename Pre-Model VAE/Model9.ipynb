{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1958fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2190281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BM MONEY\\miniconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\BM MONEY\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms as T \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "import umap\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec071f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890a0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747a9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # GLOW model parameter\n",
    "    'img_size': (128*128),\n",
    "    'in_channels': 3,\n",
    "    'hidden_channels': 512,\n",
    "    'K': 32, \n",
    "    'L': 3,\n",
    "    'coupling_layer':'affine',\n",
    "    \n",
    "    # Training parameter\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 30,\n",
    "    'weight_decay': 1e-5,\n",
    "    \n",
    "    # Clustering parameters\n",
    "    'n_cluster':3,\n",
    "    'gmm_covariance_type':'full'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e140541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a2cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = T.Compose([\n",
    "    T.Resize(config['img_size']),\n",
    "    T.RandomAdjustSharpness(0.5),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd135994",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4294901760 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(img_path) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[0;32m      8\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m         image_T \u001b[38;5;241m=\u001b[39m data_transform(image)\n\u001b[0;32m     10\u001b[0m         my_data\u001b[38;5;241m.\u001b[39mappend(image_T\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(my_data, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\BM MONEY\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\BM MONEY\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\BM MONEY\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4294901760 bytes."
     ]
    }
   ],
   "source": [
    "base_folder = os.path.join(\"..\", \"Merged Files\")\n",
    "my_data = []\n",
    "\n",
    "for i in os.listdir(base_folder):\n",
    "    img_path = os.path.join(base_folder, i)\n",
    "    \n",
    "    with Image.open(img_path) as image:\n",
    "        image = image.convert(\"RGB\")\n",
    "        image_T = data_transform(image)\n",
    "        my_data.append(image_T.unsqueeze(0))\n",
    "    os.remove(img_path)  # ลบไฟล์ที่เปิด\n",
    "    \n",
    "    # เก็บกวาด memory\n",
    "    del image_T\n",
    "    gc.collect()\n",
    "\n",
    "data = torch.cat(my_data, dim=0).to(device)\n",
    "print(f\"Dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa84ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActNorm Layer (Activation Normalization)\n",
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ActNorm, self).__init__()\n",
    "        self.loc = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "        self.scale = nn.Parameter(torch.ones(1, channels, 1, 1))\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, x):\n",
    "        with torch.no_grad():\n",
    "            flatten = x.permute(1, 0, 2, 3).contiguous().view(x.shape[1], -1)\n",
    "            mean = flatten.mean(1).view(1, x.shape[1], 1, 1)\n",
    "            std = flatten.std(1).view(1, x.shape[1], 1, 1)\n",
    "            self.loc.data.copy_(-mean)\n",
    "            self.scale.data.copy_(1 / (std + 1e-6))\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x)\n",
    "            self.initialized = True\n",
    "\n",
    "        if reverse:\n",
    "            return (x / self.scale) + self.loc, 0\n",
    "        else:\n",
    "            log_abs_det = torch.sum(torch.log(torch.abs(self.scale)), dim=1)\n",
    "            return (x - self.loc) * self.scale, log_abs_det.sum() * x.size(2) * x.size(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d87e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invertible 1x1 Convolution\n",
    "class InvConv2d(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InvConv2d, self).__init__()\n",
    "        weight = torch.qr(torch.randn(in_channels, in_channels))[0]\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        weight = self.weight\n",
    "\n",
    "        if not reverse:\n",
    "            z = F.conv2d(x, weight.view(c, c, 1, 1))\n",
    "            log_det = torch.slogdet(weight)[1] * h * w\n",
    "            return z, log_det\n",
    "        else:\n",
    "            weight_inv = torch.inverse(weight)\n",
    "            z = F.conv2d(x, weight_inv.view(c, c, 1, 1))\n",
    "            return z, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine Coupling Layer\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(AffineCoupling, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels // 2, hidden_channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, in_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        # Initialize last layer with zeros\n",
    "        self.net[-1].weight.data.zero_()\n",
    "        self.net[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        x_a, x_b = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        s_t = self.net(x_a)\n",
    "        s, t = torch.chunk(s_t, 2, dim=1)\n",
    "        s = torch.sigmoid(s + 2)  # Add +2 for numerical stability\n",
    "\n",
    "        if not reverse:\n",
    "            y_b = x_b * s + t\n",
    "            log_det = torch.sum(torch.log(s), dim=[1, 2, 3])\n",
    "        else:\n",
    "            y_b = (x_b - t) / s\n",
    "            log_det = -torch.sum(torch.log(s), dim=[1, 2, 3])\n",
    "\n",
    "        y = torch.cat([x_a, y_b], dim=1)\n",
    "\n",
    "        return y, log_det"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
